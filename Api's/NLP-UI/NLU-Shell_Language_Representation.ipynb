{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SiameseBert Semantic Sentence Similiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and \n",
    "# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BERT embedding vector - length 768\n",
      "Sample BERT embedding vector - note includes negative values [ 2.95402944e-01  2.91811317e-01  2.16480112e+00  2.20419735e-01\n",
      " -1.30864084e-02  1.01950324e+00  1.51298177e+00  2.34132320e-01\n",
      "  2.73057759e-01  1.35123059e-01 -1.11313355e+00 -1.25884756e-01\n",
      "  1.45378560e-01  9.77708519e-01  1.39352298e+00  4.57705170e-01\n",
      " -5.82131326e-01 -7.24940956e-01 -3.61734211e-01 -2.27514938e-01\n",
      "  1.66630261e-02  2.04862028e-01  6.55133009e-01 -1.29376411e+00\n",
      " -7.26099670e-01 -1.91135958e-01 -3.07211220e-01 -1.30278611e+00\n",
      " -1.42963862e+00  5.67489257e-03  3.54811579e-01  4.83713001e-01\n",
      "  6.65387988e-01  5.33848584e-01  6.40496373e-01  5.90408683e-01\n",
      "  7.83848837e-02 -1.07759190e+00 -1.24676548e-01 -3.98406267e-01\n",
      "  7.36314356e-01  5.28293312e-01  5.63290715e-01  4.14546162e-01\n",
      "  4.49179351e-01 -9.58785266e-02  1.45424581e+00 -2.69144416e-01\n",
      " -2.44059831e-01 -1.10387063e+00 -2.00923711e-01 -2.17417232e-03\n",
      "  1.83387983e+00  1.06518435e+00 -5.11946499e-01 -1.11248517e+00\n",
      "  5.59790134e-01 -5.89608967e-01  1.07621956e+00  7.49265134e-01\n",
      "  4.32666540e-01  1.76307425e-01 -1.72124021e-02  1.19170308e-01\n",
      " -8.37448120e-01  1.88446209e-01 -7.46742496e-03  1.70312636e-02\n",
      " -4.29176033e-01 -5.72340250e-01  6.22608542e-01 -3.38023782e-01\n",
      "  1.03712618e-01  2.42106840e-01 -9.82074916e-01 -2.63344109e-01\n",
      "  4.15540755e-01  2.79155582e-01 -1.03487812e-01  7.32837141e-01\n",
      "  2.78023899e-01  7.52159774e-01  8.08235168e-01 -4.87504005e-01\n",
      " -7.64051318e-01 -7.08028302e-02  5.39888501e-01  2.24033028e-01\n",
      " -1.66969872e+00 -2.01101899e-01  2.13853747e-01  1.08926690e+00\n",
      "  5.79520047e-01 -3.32589656e-01 -9.75302517e-01  2.37979591e-02\n",
      " -1.68779731e-01  2.63479173e-01  1.64727837e-01  2.03110859e-01\n",
      "  8.12724512e-03  3.75386417e-01  4.53516632e-01  4.21482213e-02\n",
      "  4.72985089e-01  3.53581369e-01 -1.55092508e-01  3.71253639e-01\n",
      "  3.54824752e-01  5.11559322e-02 -9.36767578e-01  1.42013252e+00\n",
      "  7.63064682e-01 -8.58592033e-01 -5.74660182e-01  2.43799202e-02\n",
      " -1.39292681e+00  3.14162910e-01 -2.08896790e-02  4.01286364e-01\n",
      "  8.56514752e-01  2.12013528e-01  1.93166494e-01 -8.65448341e-02\n",
      "  3.66492867e-01 -5.34847677e-01  9.08363938e-01 -1.47459939e-01\n",
      " -4.34119403e-01  1.38952702e-01 -2.85756052e-01  8.91503990e-01\n",
      " -9.49378610e-01 -6.12359270e-02 -1.63512990e-01  2.15335339e-02\n",
      "  9.33668837e-02 -1.83846056e-01 -7.05076233e-02 -9.50030237e-02\n",
      " -7.78035462e-01  5.08425295e-01 -2.38958031e-01  1.18629202e-01\n",
      " -2.22939491e-01 -4.55788553e-01  7.77876019e-01  5.58181763e-01\n",
      " -6.46435440e-01 -3.76424015e-01 -9.94760334e-01 -2.24633381e-01\n",
      "  2.56237179e-01  3.92683685e-01 -3.80206019e-01 -5.66550136e-01\n",
      "  1.06151199e+00  6.39036834e-01  2.45125800e-01 -2.10276335e-01\n",
      " -2.03608483e-01  5.58223248e-01  5.87122813e-02  4.09846961e-01\n",
      "  9.85412225e-02  5.68385348e-02 -3.91429007e-01  2.61983752e-01\n",
      "  1.85061812e-01 -7.71072149e-01  6.81888759e-01 -6.14752054e-01\n",
      " -9.09989655e-01 -5.37011325e-01 -2.67421007e-01  6.41144961e-02\n",
      " -1.47129506e-01  8.36788476e-01  2.73965210e-01  2.99047530e-01\n",
      "  1.75318450e-01 -8.45177919e-02  1.32096410e+00 -1.32269979e+00\n",
      " -8.96274745e-01 -7.21393943e-01  9.03013125e-02 -5.28525233e-01\n",
      " -5.90583794e-02 -1.73462123e-01 -1.08447242e+00 -9.68358874e-01\n",
      "  7.07607269e-01 -1.08554220e+00  7.90788233e-02 -1.23736598e-01\n",
      "  1.04828261e-01 -4.38559830e-01 -9.64855105e-02  1.21587075e-01\n",
      "  7.08318830e-01 -7.40591764e-01 -6.13735914e-02  5.97689569e-01\n",
      "  5.36595523e-01  9.54431176e-01  5.82721308e-02 -7.02313781e-01\n",
      " -5.21191776e-01 -5.31055272e-01 -6.87113285e-01 -2.28840321e-01\n",
      " -1.25094980e-01  3.82895648e-01  9.40875053e-01 -1.53620017e+00\n",
      "  3.08551490e-01 -1.07018912e+00 -9.22978669e-02  1.83376998e-01\n",
      "  2.36132532e-01 -9.44777608e-01  3.63419354e-01  1.02014877e-01\n",
      " -2.61932820e-01  1.31475377e+00 -8.66959393e-02  4.19598281e-01\n",
      " -5.64484954e-01 -2.40892500e-01  9.56284702e-02  2.88403362e-01\n",
      " -1.26559591e+00  9.68207270e-02 -6.86096668e-01 -9.73237216e-01\n",
      "  4.98170078e-01 -6.72291636e-01  6.18023753e-01 -2.36298636e-01\n",
      " -1.19644068e-02  2.23921582e-01  1.29233623e+00  9.04313743e-01\n",
      " -6.94557071e-01  2.12914318e-01  1.41126603e-01 -1.09073710e+00\n",
      " -2.61027038e-01  3.72416139e-01  5.65628409e-01 -7.63028085e-01\n",
      "  2.54305422e-01  2.96842158e-01  6.39281929e-01 -3.68397146e-01\n",
      " -1.50474697e-01 -2.06996277e-01  4.89719212e-01 -1.07649374e+00\n",
      " -4.69321907e-01  5.64345896e-01 -8.54325294e-01  2.01307178e-01\n",
      " -3.64126831e-01 -4.83238459e-01 -2.99138218e-01 -2.37890914e-01\n",
      " -7.14594483e-01  3.88848297e-02  2.88148999e-01 -8.43381107e-01\n",
      " -1.32528722e-01  3.20764095e-01 -5.64771235e-01 -6.50783837e-01\n",
      "  1.23112655e+00 -3.79267842e-01 -5.36719561e-01 -1.18963338e-01\n",
      "  5.57513356e-01  7.38234639e-01 -1.65188766e+00  4.81853247e-01\n",
      " -9.48715091e-01  9.54266369e-01  6.12537861e-01  2.52185971e-01\n",
      " -2.26502582e-01 -1.06310524e-01 -6.21923029e-01  7.16752172e-01\n",
      " -3.86202514e-01 -7.54138887e-01  5.86001277e-01 -2.63745040e-01\n",
      "  2.03416273e-01 -5.31250656e-01 -9.58163261e-01  7.24046767e-01\n",
      " -6.24142051e-01 -1.32546574e-01 -7.14519799e-01  2.18565777e-01\n",
      " -6.77283883e-01 -1.44099548e-01 -3.61371100e-01  8.55807662e-01\n",
      "  4.30025756e-01  5.20425856e-01 -1.25135803e+00  8.63286201e-03\n",
      " -8.13418508e-01 -1.89724296e-01  6.97539210e-01 -2.71261543e-01\n",
      " -9.60209846e-01 -5.67386985e-01 -1.97102338e-01 -8.45597148e-01\n",
      "  4.09595966e-01  4.65882480e-01  1.00615904e-01  1.79212287e-01\n",
      "  3.18223447e-01 -1.94157317e-01 -2.31448114e-02  5.31907439e-01\n",
      "  2.07857043e-01 -1.38748690e-01 -1.25763699e-01 -1.67028785e+00\n",
      "  7.72699714e-02  3.29649836e-01  9.59620833e-01  9.71129537e-01\n",
      " -8.59887481e-01 -7.97947466e-01 -4.50175226e-01  6.09350502e-01\n",
      " -9.92596895e-02 -1.26160514e+00 -4.80853081e-01 -4.37264778e-02\n",
      "  7.61089921e-01 -4.90454376e-01 -1.16635025e+00 -1.26680589e+00\n",
      "  4.23250139e-01 -3.28321695e-01  1.13249160e-01  1.19094706e+00\n",
      " -4.76775944e-01  2.77292550e-01 -6.62052691e-01 -6.26967311e-01\n",
      " -3.28042984e-01  4.62100416e-01  4.22706977e-02  1.11227203e-02\n",
      "  3.49017859e-01  3.41067016e-01 -2.20341682e-01 -8.18508327e-01\n",
      "  6.06628418e-01 -6.28642440e-01  2.82574236e-01  4.81880993e-01\n",
      "  1.42555857e+00 -1.52597949e-03 -5.17759204e-01 -2.46804468e-02\n",
      " -5.39777815e-01  2.85306156e-01 -2.21777350e-01 -1.85012788e-01\n",
      "  3.11429441e-01 -2.07644746e-01 -2.50214636e-01 -9.53149199e-01\n",
      "  4.12619203e-01  3.92303020e-02  1.41120583e-01 -3.55186492e-01\n",
      "  1.18068111e+00 -3.14618289e-01  4.79557037e-01 -4.70073760e-01\n",
      "  1.77134842e-01  6.75846934e-01  9.84488130e-01  2.68136650e-01\n",
      " -7.01521616e-03 -2.85394281e-01 -1.52261078e-01 -1.11626446e+00\n",
      "  1.02339134e-01  5.77064395e-01  3.41381997e-01  4.19948757e-01\n",
      "  4.03285176e-01  2.95105428e-01  9.24367070e-01  1.26249290e+00\n",
      "  1.20447159e+00  9.47556794e-01  4.87715192e-02  4.61912960e-01\n",
      " -8.18414807e-01  4.46733654e-01  6.60864770e-01  4.44435447e-01\n",
      " -9.46657538e-01  3.16685319e-01 -5.51689148e-01 -1.82610437e-01\n",
      " -1.46252930e-01 -1.01644242e+00  7.33199239e-01  1.08742750e+00\n",
      " -6.23245060e-01 -6.87371254e-01 -1.63677678e-01  2.05873683e-01\n",
      " -1.28421190e-04  1.58518386e+00 -2.15279385e-01 -3.93540204e-01\n",
      " -2.25849748e-01 -4.96896729e-02 -3.16225260e-01 -2.08421946e-02\n",
      " -7.36599326e-01  5.96843243e-01 -6.45120502e-01 -5.47669351e-01\n",
      "  1.46693066e-01 -1.00796354e+00  2.46947974e-01 -1.16965070e-01\n",
      "  1.06088793e+00  1.71403643e-02  1.60032064e-01 -1.61019713e-02\n",
      " -4.65260088e-01 -3.54802281e-01 -7.89501429e-01 -5.38767338e-01\n",
      "  4.41756874e-01 -1.14355065e-01  2.15809301e-01  3.88404101e-01\n",
      " -5.96565187e-01  6.83732271e-01  1.11033106e+00  8.61530423e-01\n",
      " -1.48576662e-01  1.11748528e+00  3.42844367e-01 -8.33451301e-02\n",
      "  3.36355492e-02  3.10089856e-01 -1.54736972e+00 -4.20449257e-01\n",
      "  8.92352611e-02 -1.90214649e-01 -7.60474801e-02 -9.25956368e-01\n",
      " -2.31246069e-01  3.78084004e-01 -9.29461360e-01 -2.05470532e-01\n",
      " -1.10454954e-01  2.51402408e-01  8.28983337e-02  4.65330511e-01\n",
      " -1.59002984e+00  1.67837087e-02 -1.03855550e-01  4.42655414e-01\n",
      "  5.59934497e-01  4.51579660e-01 -7.67549649e-02 -4.73889410e-01\n",
      " -1.16150522e+00  2.94714987e-01  2.40399048e-01  3.64251643e-01\n",
      "  5.29475093e-01  2.42407480e-03  1.62732545e-02 -1.22207187e-01\n",
      " -1.02022302e+00 -1.01030040e+00 -3.02591741e-01 -2.43044734e-01\n",
      " -7.07274795e-01  1.77668303e-01  2.10999250e-01  7.66401172e-01\n",
      "  3.03304940e-01 -9.95253306e-03 -5.04403055e-01 -5.96487880e-01\n",
      "  4.28530127e-01  7.56300762e-02  1.18148148e+00  2.74923202e-02\n",
      "  1.09997904e+00  1.69377506e-01  1.15801372e-01  7.44199574e-01\n",
      " -1.96190372e-01 -5.51070809e-01 -3.15140724e-01 -7.55082667e-01\n",
      " -5.86988807e-01  4.44423676e-01 -3.26272428e-01 -5.73182821e-01\n",
      " -4.43319142e-01  3.59878778e-01 -4.29274067e-02  8.31856132e-01\n",
      "  6.22971654e-01 -2.17173502e-01 -9.20350194e-01  9.19905663e-01\n",
      " -7.29423165e-02  5.15981689e-02 -9.65986103e-02 -1.27199337e-01\n",
      " -3.96581501e-01  4.10285965e-02  2.23800272e-01  2.97161400e-01\n",
      "  2.92265654e-01 -5.09368539e-01 -5.68877280e-01  4.27781582e-01\n",
      "  4.16459411e-01  4.65236723e-01  1.06364703e+00 -6.52427673e-01\n",
      " -8.28918755e-01  3.47215943e-02  3.42104584e-01  1.88819274e-01\n",
      "  5.04421651e-01 -2.34062001e-01  1.80037275e-01  4.29086506e-01\n",
      "  2.30355427e-01 -2.83437192e-01  2.34264776e-01 -5.11488318e-01\n",
      "  4.76539075e-01  1.56475931e-01 -1.47166729e-01 -1.02309179e+00\n",
      " -5.59814334e-01 -3.14807892e-01  1.36169437e-02  2.48336554e-01\n",
      " -5.14447451e-01 -9.95984077e-01  2.44895406e-02 -2.81418804e-02\n",
      "  2.93588519e-01 -6.17680669e-01  2.71538824e-01 -6.88656986e-01\n",
      "  5.12018085e-01 -5.74778803e-02 -2.89483696e-01 -2.38753229e-01\n",
      "  5.62781274e-01 -1.02241445e+00 -6.51534140e-01  2.27025792e-01\n",
      "  3.18955362e-01  8.17962512e-02 -5.93583882e-02 -1.17566586e+00\n",
      " -4.10226025e-02 -2.96685010e-01  5.82193553e-01 -7.70743489e-01\n",
      " -7.15864971e-02  4.76809084e-01  3.29380214e-01 -5.99578917e-01\n",
      "  5.35222283e-03 -3.21955323e-01  1.17017007e+00 -1.79935142e-01\n",
      " -5.21227479e-01 -2.49459669e-01  4.84305680e-01 -2.64465004e-01\n",
      " -2.03607604e-01  3.04992646e-01 -1.25714993e+00  7.55809128e-01\n",
      " -6.03417039e-01  8.71125460e-02 -1.06221035e-01 -5.40654898e-01\n",
      " -5.11184156e-01 -4.32207845e-02  6.84743345e-01 -9.05072570e-01\n",
      "  4.51823771e-02 -2.63025790e-01 -7.73667753e-01 -7.93841839e-01\n",
      " -3.95873219e-01  2.98926532e-02  1.12445629e+00  8.02643120e-01\n",
      " -3.85726929e-01 -7.29575813e-01  6.11615181e-01 -1.75381273e-01\n",
      " -1.70759574e-01 -1.05162561e+00  1.05050993e+00 -6.94110394e-01\n",
      "  6.68202937e-01 -1.40023589e-01  4.58647013e-01  5.88180609e-02\n",
      "  3.39239091e-01  2.20731661e-01 -4.91490602e-01  8.13486218e-01\n",
      "  1.32252121e+00  1.83043271e-01  2.82913834e-01 -4.13827658e-01\n",
      "  2.75093406e-01 -1.03680050e+00  4.73541588e-01  6.67740107e-01\n",
      " -7.56590292e-02  9.01939422e-02 -8.08835149e-01 -1.11947414e-02\n",
      " -4.65077572e-02  7.15482235e-01 -1.27313048e-01 -6.92540526e-01\n",
      " -7.13322461e-01  2.09888667e-01 -3.56735677e-01 -2.66319066e-01\n",
      " -4.60457981e-01  7.38279242e-03 -8.48066509e-01 -8.13464820e-01\n",
      "  2.13946104e-01 -5.31533420e-01 -8.76830593e-02  3.71387184e-01\n",
      "  4.14128959e-01  8.61194253e-01  8.82490128e-02  3.36408108e-01\n",
      "  8.24473128e-02  1.00937819e+00  1.82786688e-01  4.32672501e-02\n",
      "  1.73484862e-01 -1.22647095e+00 -1.27435967e-01 -7.31792688e-01\n",
      "  9.08650398e-01 -8.02065074e-01  2.60243058e-01  8.27427059e-02\n",
      "  1.52959570e-01  1.81249827e-01 -7.75715888e-01  3.96856099e-01\n",
      "  6.91959739e-01 -2.00028688e-01 -1.84013397e-01  7.82086551e-02\n",
      " -4.08993810e-01 -3.83004360e-02  8.61752689e-01 -2.74388075e-01\n",
      " -9.74844843e-02 -1.02602744e+00 -1.26833364e-01  1.11962521e+00\n",
      "  6.53883994e-01  1.42315060e-01 -4.75913584e-01 -3.21262144e-02\n",
      " -2.65989065e-01 -1.16977870e-01 -5.93443394e-01  6.56192422e-01\n",
      " -5.73521256e-01 -7.07489491e-01 -7.29676843e-01  6.20012125e-03\n",
      "  6.79842770e-01  4.15460676e-01 -3.41879636e-01  2.28170466e+00\n",
      " -1.02978492e+00  4.97357517e-01 -4.68666613e-01  1.19645047e+00\n",
      " -6.38581753e-01 -1.35883927e-01 -7.98879266e-01 -5.45745015e-01\n",
      " -4.36387628e-01 -4.08294767e-01 -2.27469057e-01 -1.78951770e-01\n",
      "  5.60197458e-02  2.35111624e-01 -8.29315186e-01  1.66119989e-02\n",
      " -8.07582080e-01 -6.34335458e-01  3.33199948e-01  2.99568027e-01\n",
      " -8.49243164e-01  5.52446730e-02  8.02987397e-01  4.63941187e-01\n",
      "  6.54758453e-01 -5.36930573e-04 -2.04843760e-01  9.76368606e-01\n",
      " -3.53015989e-01  9.11225915e-01  7.22659111e-01 -3.19765866e-01\n",
      "  2.27475837e-01 -2.76821256e-01  4.26936448e-01  3.22493732e-01\n",
      " -4.22099888e-01  2.96952069e-01  6.81850493e-01  1.47002411e+00\n",
      " -2.75382064e-02 -7.22703576e-01  3.05490252e-02 -5.42970784e-02\n",
      " -5.06122112e-01  4.57349941e-02  8.26342031e-02  4.99085993e-01\n",
      "  9.00192857e-01 -8.83195400e-01 -9.96072114e-01 -2.98155248e-01\n",
      " -4.14106607e-01 -5.26974440e-01 -5.91103196e-01 -2.92363346e-01]\n"
     ]
    }
   ],
   "source": [
    "# A corpus is a list with documents split by sentences.\n",
    "sentences = ['weather','clouds','time','sun','rain','dust','strom','winds']\n",
    "sentence = ['Absence of sanity', \n",
    "             'Lack of saneness',\n",
    "             'A man is eating food.',\n",
    "             'A man is eating a piece of bread.',\n",
    "             'The girl is carrying a baby.',\n",
    "             'A man is riding a horse.',\n",
    "             'A woman is playing violin.',\n",
    "             'Two men pushed carts through the woods.',\n",
    "             'A man is riding a white horse on an enclosed ground.',\n",
    "             'A monkey is playing drums.',\n",
    "             'A cheetah is running behind its prey.',\n",
    "             'Get me updates on stock market',\n",
    "             'Get me updates on rupee market',\n",
    "             \"get me details of\",\n",
    "            \"1. Abstractive Model\",\n",
    "            \"2. Extractive Model\",\n",
    "             \"linux\",\n",
    "            \"Launch\",\n",
    "            \"Build\"]\n",
    "\n",
    "# Each sentence is encoded as a 1-D vector with 78 columns\n",
    "sentence_embeddings = model.encode(sentence)\n",
    "\n",
    "print('Sample BERT embedding vector - length', len(sentence_embeddings[0]))\n",
    "\n",
    "print('Sample BERT embedding vector - note includes negative values', sentence_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_utils_base:Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search Results\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: rocket\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Launch (Cosine Score: 0.7751)\n",
      "Build (Cosine Score: 0.5190)\n",
      "linux (Cosine Score: 0.5184)\n",
      "1. Abstractive Model (Cosine Score: 0.4717)\n",
      "get me details of (Cosine Score: 0.4658)\n"
     ]
    }
   ],
   "source": [
    "#@title Sematic Search Form\n",
    "\n",
    "# code adapted from https://github.com/UKPLab/sentence-transformers/blob/master/examples/application_semantic_search.py\n",
    "import scipy\n",
    "#query = \"second one\" #@param {type: 'string'}\n",
    "query = \"rocket\"\n",
    "\n",
    "queries = [query]\n",
    "query_embeddings = model.encode(queries)\n",
    "\n",
    "# Find the closest 3 sentences of the corpus for each query sentence based on cosine similarity\n",
    "number_top_matches = 5 #@param {type: \"number\"}\n",
    "\n",
    "print(\"Semantic Search Results\")\n",
    "\n",
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0]\n",
    "\n",
    "    results = zip(range(len(distances)), distances)\n",
    "    results = sorted(results, key=lambda x: x[1])\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for idx, distance in results[0:number_top_matches]:\n",
    "        print(sentence[idx].strip(), \"(Cosine Score: %.4f)\" % (1-distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
