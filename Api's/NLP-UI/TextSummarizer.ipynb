{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from summarizer import Summarizer\n",
    "from summarizer.coreference_handler import CoreferenceHandler\n",
    "from transformers import *\n",
    "\n",
    "body = '''despite the fact that tony stark does not have any superpowers he has an almost superhuman ability to predict problems in the future and proactively form plans to solve those problems after loki was able to successfully manipulate the hulk into rampaging and nearly destroying the shield helle carrier during the events of avengers it became obvious that tony and the rest of the avengers needed a better contingency plan for the big guy if he ever got out of control again during iron man three we see that iron ban had begun to start developing a series of more specialized armors for tackling specific scenarios one of those souths is seen during the events of age of ultronn when scarlet which is able to force halt in a transforming and rampaging through to hannesburg south africa starkas force to deployed the mark forty four a k a the hullcluster armor today's bidil is going to cover everything we know about this thick armour and its capabilities now before we begin the version of the hawkbuster armor that we're discussing to day is the version from the m c u and not from the comics as with our other lower videos it's important to make this distinction because the m c u is not the main comic book six one six universe and as most of us longtime comic readers know the m c u takes creative liberties with a lot of the source material so jest keep that in mind were only talking about the halkbuster from the films and supplemental film material the mark forty four armour begain development presumably after the events of avengers as a collaborative effort between tony stark and bruce banner when bruce transforms into the hulk he has described the control of halks's actions as a steering wheel where hulk and bruce both have a hand on the wheel however when forced to change in the avengers by loki and again in age of altron by scarlet whitch hulk takes over and begins to rampage causing complete havoc bruce helps tony develop the mark forty four whose sole purpose is to take on the halt in the event that he becomes inconsolable the holtbuster armors deployed via a mobile service delivery module that can be deployed anywhere in the world named veronica director josweed named the system veronica after the archi comics character of the same name veronica was a rival to archie's girlfriend betty jos said i just decided to call the armor of veronica because banner used to be in love with a girl named betty and veronicas the opposite of that veronica is essentially a giant mobile floating armory that houses the mark forty four as well as replacement parts for the haltbuster armor unit when tony calls for veronicas aid the holkbuster armors deployed and rockets its way to him this is a clear evolution of the teck that tony was working on to some degree of success in ironman three in the mark forty two k a the prodigal sun armor the halbuster armour is considered modular in the sense that the armors parts can be switched and replaced on the fly it was presumably designed this way to account for the fact that any fight with the hulk would surely damage the unit to the point where it would need to have replacement parts swamped in and out this is evidenced in the fight when the halk rans a traffic light pole through the suits left arm just below the shoulder and the hawkbuster atjacks the armor and installs a new one in place continuing to allow it start to do battle at banner in order to generate the significantly improved strength at the unit boasted the armor had to be powered by multiple arcreactors there is one in the center of the chest one on the back of each leg two on the front of each leg four on the back near the propulsion system three on each lat one under each shoulder paldron and one on the palm of each hand all of this for a total of at least twenty one archreactors that we could count the strength of the armour was able to consistently match the rampaging halt strength as well as his striking power the suit was strong enough to send halk flying down a street in one punch as well as strong enough to knock his tooth out when combined with the propulsion system it's even able to body slam hulk hold him down firmly in place and land a series of consecutive plunches to the head ve a jack hammer like hydraulic battering rain mechanism embedded in the arms the suits enhanced durability is also consistently tested throughout the battle the suit features a reinforced exoskeleton that helps protect tony inside of the suit halk throws cars at it shoulder charges it punches it kicks it rips off or completely destroys multiple various pieces of the souit and hulk even throws it through various buildings and just generally wails on this thing pretty hard the suit is capable of flight through a mixture of the archreactors on its back as well as its jet propulsion system ar creactors on the palm of the hands are capable of firing unibeams similar to what iron men has been able to do with other armors except presumably much stronger it has a really cool locking mechanism built into the arms that allow the hand to retract and lock the halt's arm within it and use it for restraint the suit also has a high power chemical sendative spray built into it that did absolutely nothing against the hawk but hey it is the thought that counts the suit also has various missiles built into it and tony uses those missiles to weaken the structural integrity of a building that he ultimately destroys when crashing the hulk through it the suit also features an ai within it similar to jarvis but its full ability set is unknown at this point in the m c u jarvis was presumed destroyed by altron we know its capable of delivering commands to veronica feeding diagnostics to the suit backto tony compiling damage reports scanning large areas and structures for potential threats and casualties in purchasing whole buildings to destroy ultimately the suit proved successful in stopping the hulk from rampaging in johannesburg even though it came with a tremendous amount of damage in the city the mark forty four is eventually improved upon in the form of the mark forty eight or the halkbuster two point o the hullcluster two features the same modular design and multiple arch reactors with a somewhat more streamlined appearance the suit was first used in the battleo l conda where it destroyed a number of outriders as well as did battle with the mass of colebsidian this holtbuster was having a difficult time dealing with ubsidian but was able to defeat and kill him when banner tricked him in distabbing one of the halkbusters broken arms then triggered the propulsion system on the arm which forced the behemeth into a conda's protective dome causing a mass of explosion we can assume that the suit was physically stronger and more durable than the previous model because tony and bruce would have learned from the battle of johannesburg and improved upon the model in every way outside of that the second interation of the hawkbuster was only used one more time during the capture of danas on his vacation planet danos was significantly weakened at that point so the suit was more than capable of restraining him until ultimately he was killed the holkbuster suit is definitely one of the coolest armors that wehave seen in the m c u and considering the pantheon of awesome armors that tony has that sang quite a lot thank you gus for watching this video we hope you learned a lot about the thickest battle armour of them all the halkbuster and if you did please consider subscribing and dropping ha like on this video better yet drop a comment down below letting us know what other suits of armors you like us to cover on the channel and maybe we can eventually cover them all this has been nicke with key issues and remember the motto hawkbusters over everything'''\n",
    "\n",
    "custom_config = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=custom_config)\n",
    "\n",
    "# handler = CoreferenceHandler(greedyness=.4)\n",
    "model = Summarizer(custom_model = custom_model, custom_tokenizer=custom_tokenizer)\n",
    "\n",
    "model(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building. Walter Chrysler had set out to build the tallest building in the world, a competition at that time with another Manhattan skyscraper under construction at 40 Wall Street at the south end of Manhattan.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = '''\n",
    "The Chrysler Building, the famous art deco New York skyscraper, will be sold for a small fraction of its previous sales price.\n",
    "The deal, first reported by The Real Deal, was for $150 million, according to a source familiar with the deal.\n",
    "Mubadala, an Abu Dhabi investment fund, purchased 90% of the building for $800 million in 2008.\n",
    "Real estate firm Tishman Speyer had owned the other 10%.\n",
    "The buyer is RFR Holding, a New York real estate company.\n",
    "Officials with Tishman and RFR did not immediately respond to a request for comments.\n",
    "It's unclear when the deal will close.\n",
    "The building sold fairly quickly after being publicly placed on the market only two months ago.\n",
    "The sale was handled by CBRE Group.\n",
    "The incentive to sell the building at such a huge loss was due to the soaring rent the owners pay to Cooper Union, a New York college, for the land under the building.\n",
    "The rent is rising from $7.75 million last year to $32.5 million this year to $41 million in 2028.\n",
    "Meantime, rents in the building itself are not rising nearly that fast.\n",
    "While the building is an iconic landmark in the New York skyline, it is competing against newer office towers with large floor-to-ceiling windows and all the modern amenities.\n",
    "Still the building is among the best known in the city, even to people who have never been to New York.\n",
    "It is famous for its triangle-shaped, vaulted windows worked into the stylized crown, along with its distinctive eagle gargoyles near the top.\n",
    "It has been featured prominently in many films, including Men in Black 3, Spider-Man, Armageddon, Two Weeks Notice and Independence Day.\n",
    "The previous sale took place just before the 2008 financial meltdown led to a plunge in real estate prices.\n",
    "Still there have been a number of high profile skyscrapers purchased for top dollar in recent years, including the Waldorf Astoria hotel, which Chinese firm Anbang Insurance purchased in 2016 for nearly $2 billion, and the Willis Tower in Chicago, which was formerly known as Sears Tower, once the world's tallest.\n",
    "Blackstone Group (BX) bought it for $1.3 billion 2015.\n",
    "The Chrysler Building was the headquarters of the American automaker until 1953, but it was named for and owned by Chrysler chief Walter Chrysler, not the company itself.\n",
    "Walter Chrysler had set out to build the tallest building in the world, a competition at that time with another Manhattan skyscraper under construction at 40 Wall Street at the south end of Manhattan. He kept secret the plans for the spire that would grace the top of the building, building it inside the structure and out of view of the public until 40 Wall Street was complete.\n",
    "Once the competitor could rise no higher, the spire of the Chrysler building was raised into view, giving it the title.\n",
    "'''\n",
    "\n",
    "\n",
    "model(body, min_length =150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Before the emergence of the Wakanda nation, mystic beings known as Originators were expelled from the region by the Heliopolitans.[9] This was later retconned as the Orisha,[10] the pantheon of Wakanda consisting of Thoth, Ptah, Mujaji, Kokou and Bast, the Panther Goddess.[11][12]\\n\\nIn the distant past, a massive meteorite made up of the element vibranium crashed in Wakanda. It was unearthed a generation before the events of the present day. Despite losses, the Wakandans defeat the Skrulls. A warning against invading Wakanda is left written on the wall of the ship's control center.[14]\\n\\nWhile under the cosmic power of the Phoenix Force, Namor attacks Wakanda for hiding the Avengers and destroys much of the country with a tidal wave.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body1 = '''\n",
    "Before the emergence of the Wakanda nation, mystic beings known as Originators were expelled from the region by the Heliopolitans.[9] This was later retconned as the Orisha,[10] the pantheon of Wakanda consisting of Thoth, Ptah, Mujaji, Kokou and Bast, the Panther Goddess.[11][12]\n",
    "\n",
    "In the distant past, a massive meteorite made up of the element vibranium crashed in Wakanda. It was unearthed a generation before the events of the present day. T'Challa, the current Black Panther, is the son of T'Chaka, the Black Panther before him and a descendant of Bashenga. Knowing that others would attempt to manipulate and dominate Wakanda for this rare and valuable resource, T'Chaka conceals his country from the outside world. He sells off minute amounts of the valuable vibranium while surreptitiously sending the country's best scholars to study abroad, consequently turning Wakanda into one of the world's most technologically advanced nations. Eventually, however, the explorer Ulysses Klaue finds his way to Wakanda and covers up his work on a vibranium-powered, sound-based weapon. When exposed, Klaue kills T'Chaka, only to see his \"sound blaster\" turned on him by a grieving teenaged T'Challa. Klaue's right hand is destroyed, and he and his men flee.[13]\n",
    "\n",
    "Wakanda has an unusually high rate of mutation due to the dangerously mutagenic properties of the Vibranium Mound. A large number of these Wakandan Mutates are working for Erik Killmonger.[13]\n",
    "\n",
    "Vibranium radiation has permeated much of Wakanda's flora and fauna, including the Heart-Shaped Herb eaten by members of the Black Panther Cult (although T'Challa once allowed a dying Spider-Man to eat it in the hope that it would help him deal with a mysterious illness) and the flesh of the White Gorilla eaten by the members of the White Gorilla Cult.\n",
    "\n",
    "In the 2008 \"Secret Invasion\" storyline, Skrull forces led by Commander K'vvvr invade Wakanda and engage Black Panther and his forces. Due to heavy resistance to the deployment of technological developments, both sides are forced to fight with swords and spears. The Wakandan forces voluntarily wear panther masks; this prevents the Skrulls from focusing attacks on their leader. Despite losses, the Wakandans defeat the Skrulls. They kill every single one, including K'vvvr, and send their ship back, packed with the bodies. A warning against invading Wakanda is left written on the wall of the ship's control center.[14]\n",
    "\n",
    "While under the cosmic power of the Phoenix Force, Namor attacks Wakanda for hiding the Avengers and destroys much of the country with a tidal wave. After the attack, all mutants are banned from Wakanda as stated by Black Panther and attack on some students from the Jean Grey school by its people who barely flee with the help of Storm.[15]\n",
    "'''\n",
    "\n",
    "model(body1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One of the biggest challenges in natural language processing (NLP) is the shortage of training data. To help close this gap in data, researchers have developed a variety of techniques for training general purpose language representation models using the enormous amount of unannotated text on the web (known as pre-training). In our associated paper, we demonstrate state-of-the-art results on 11 NLP tasks, including the very competitive Stanford Question Answering Dataset (SQuAD v1.1). For example, the word “bank” would have the same context-free representation in “bank account” and “bank of the river.” However, it is not possible to train bidirectional models by simply conditioning each word on its previous and next words, since this would allow the word that’s being predicted to indirectly “see itself” in a multi-layer model. BERT also learns to model relationships between sentences by pre-training on a very simple task that can be generated from any text corpus: Given two sentences A and B, is B the actual next sentence that comes after A in the corpus, or just a random sentence? For example:\\n\\nTraining with Cloud TPUs\\nEverything that we’ve described so far might seem fairly straightforward, so what’s the missing piece that made it work so well? Alternatively, you can get started using BERT through Colab with the notebook “BERT FineTuning with Cloud TPUs.”'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body2 = '''\n",
    "One of the biggest challenges in natural language processing (NLP) is the shortage of training data. Because NLP is a diversified field with many distinct tasks, most task-specific datasets contain only a few thousand or a few hundred thousand human-labeled training examples. However, modern deep learning-based NLP models see benefits from much larger amounts of data, improving when trained on millions, or billions, of annotated training examples. To help close this gap in data, researchers have developed a variety of techniques for training general purpose language representation models using the enormous amount of unannotated text on the web (known as pre-training). The pre-trained model can then be fine-tuned on small-data NLP tasks like question answering and sentiment analysis, resulting in substantial accuracy improvements compared to training on these datasets from scratch.\n",
    "\n",
    "This week, we open sourced a new technique for NLP pre-training called Bidirectional Encoder Representations from Transformers, or BERT. With this release, anyone in the world can train their own state-of-the-art question answering system (or a variety of other models) in about 30 minutes on a single Cloud TPU, or in a few hours using a single GPU. The release includes source code built on top of TensorFlow and a number of pre-trained language representation models. In our associated paper, we demonstrate state-of-the-art results on 11 NLP tasks, including the very competitive Stanford Question Answering Dataset (SQuAD v1.1).\n",
    "\n",
    "What Makes BERT Different?\n",
    "BERT builds upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, and ULMFit. However, unlike these previous models, BERT is the first deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus (in this case, Wikipedia).\n",
    "\n",
    "Why does this matter? Pre-trained representations can either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same context-free representation in “bank account” and “bank of the river.” Contextual models instead generate a representation of each word that is based on the other words in the sentence. For example, in the sentence “I accessed the bank account,” a unidirectional contextual model would represent “bank” based on “I accessed the” but not “account.” However, BERT represents “bank” using both its previous and next context — “I accessed the ... account” — starting from the very bottom of a deep neural network, making it deeply bidirectional.\n",
    "\n",
    "A visualization of BERT’s neural network architecture compared to previous state-of-the-art contextual pre-training methods is shown below. The arrows indicate the information flow from one layer to the next. The green boxes at the top indicate the final contextualized representation of each input word:\n",
    "\n",
    "BERT is deeply bidirectional, OpenAI GPT is unidirectional, and ELMo is shallowly bidirectional.\n",
    "The Strength of Bidirectionality\n",
    "If bidirectionality is so powerful, why hasn’t it been done before? To understand why, consider that unidirectional models are efficiently trained by predicting each word conditioned on the previous words in the sentence. However, it is not possible to train bidirectional models by simply conditioning each word on its previous and next words, since this would allow the word that’s being predicted to indirectly “see itself” in a multi-layer model.\n",
    "\n",
    "To solve this problem, we use the straightforward technique of masking out some of the words in the input and then condition each word bidirectionally to predict the masked words. For example:\n",
    "\n",
    "While this idea has been around for a very long time, BERT is the first time it was successfully used to pre-train a deep neural network.\n",
    "\n",
    "BERT also learns to model relationships between sentences by pre-training on a very simple task that can be generated from any text corpus: Given two sentences A and B, is B the actual next sentence that comes after A in the corpus, or just a random sentence? For example:\n",
    "\n",
    "Training with Cloud TPUs\n",
    "Everything that we’ve described so far might seem fairly straightforward, so what’s the missing piece that made it work so well? Cloud TPUs. Cloud TPUs gave us the freedom to quickly experiment, debug, and tweak our models, which was critical in allowing us to move beyond existing pre-training techniques. The Transformer model architecture, developed by researchers at Google in 2017, also gave us the foundation we needed to make BERT successful. The Transformer is implemented in our open source release, as well as the tensor2tensor library.\n",
    "\n",
    "Results with BERT\n",
    "To evaluate performance, we compared BERT to other state-of-the-art NLP systems. Importantly, BERT achieved all of its results with almost no task-specific changes to the neural network architecture. On SQuAD v1.1, BERT achieves 93.2% F1 score (a measure of accuracy), surpassing the previous state-of-the-art score of 91.6% and human-level score of 91.2%:\n",
    "\n",
    "BERT also improves the state-of-the-art by 7.6% absolute on the very challenging GLUE benchmark, a set of 9 diverse Natural Language Understanding (NLU) tasks. The amount of human-labeled training data in these tasks ranges from 2,500 examples to 400,000 examples, and BERT substantially improves upon the state-of-the-art accuracy on all of them:\n",
    "\n",
    "Making BERT Work for You\n",
    "The models that we are releasing can be fine-tuned on a wide variety of NLP tasks in a few hours or less. The open source release also includes code to run pre-training, although we believe the majority of NLP researchers who use BERT will never need to pre-train their own models from scratch. The BERT models that we are releasing today are English-only, but we hope to release models which have been pre-trained on a variety of languages in the near future.\n",
    "\n",
    "The open source TensorFlow implementation and pointers to pre-trained BERT models can be found at http://goo.gl/language/bert. Alternatively, you can get started using BERT through Colab with the notebook “BERT FineTuning with Cloud TPUs.”\n",
    "\n",
    "You can also read our paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" for more details.\n",
    "\n",
    "'''\n",
    "\n",
    "model(body2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. We call this architecture a Pointer Net (Ptr-Net). Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body3 = '''\n",
    "We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in each step of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorial optimization problems belong to this class. Our model solves the problem of variable size output dictionaries using a recently proposed mechanism of neural attention. It differs from the previous attention attempts in that, instead of using attention to blend hidden units of an encoder to a context vector at each decoder step, it uses attention as a pointer to select a member of the input sequence as the output. We call this architecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn approximate solutions to three challenging geometric problems -- finding planar convex hulls, computing Delaunay triangulations, and the planar Travelling Salesman Problem -- using training examples alone. Ptr-Nets not only improve over sequence-to-sequence with input attention, but also allow us to generalize to variable size output dictionaries. We show that the learnt models generalize beyond the maximum lengths they were trained on. We hope our results on these tasks will encourage a broader exploration of neural learning for discrete problems.\n",
    "'''\n",
    "\n",
    "model(body3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
